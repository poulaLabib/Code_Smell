"""
ğŸš€ ULTIMATE PRODUCTION MODEL v2.0
=================================
This version adds:
  âœ… XGBoost (better than RF for imbalanced data)
  âœ… Stacking Ensemble (learns optimal combination)
  âœ… Cross-validation for reliability
  âœ… Threshold tuning per class
  âœ… Confidence calibration

Target: Macro F1 > 0.80

Run:
    python ultimate_model.py
"""

import json
import os
import numpy as np
import pandas as pd
from sklearn.ensemble import (
    RandomForestClassifier, 
    GradientBoostingClassifier,
    StackingClassifier,
    VotingClassifier
)
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.utils.class_weight import compute_class_weight
from sklearn.calibration import CalibratedClassifierCV
import joblib
import warnings
warnings.filterwarnings('ignore')

# Try to import XGBoost
try:
    from xgboost import XGBClassifier
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("âš ï¸  XGBoost not installed. Run: pip install xgboost")

BASE = os.path.dirname(os.path.abspath(__file__))
SMELLS = ["Clean", "DataClass", "DeadCode", "FeatureEnvy", "GodClass", "LongMethod"]
smell_to_idx = {s: i for i, s in enumerate(SMELLS)}
idx_to_smell = {i: s for i, s in enumerate(SMELLS)}

# CK metric columns
CK_COLS = ['LOC', 'WMC', 'METHODS', 'FIELDS', 'PRIVATE_METHODS', 
           'CBO', 'DIT', 'LCOM', 'TCC', 'ATFD', 'MAX_METHOD_LOC', 'NOC']

def extract_ck_features(row):
    return [float(row.get(col, 0) or 0) for col in CK_COLS]

def add_derived_features(X):
    """Add engineered features for better smell detection."""
    X_new = []
    for row in X:
        features = list(row)
        
        # Base metrics with safety
        loc = max(row[CK_COLS.index('LOC')], 1)
        wmc = max(row[CK_COLS.index('WMC')], 1)
        methods = max(row[CK_COLS.index('METHODS')], 1)
        fields = max(row[CK_COLS.index('FIELDS')], 0)
        cbo = max(row[CK_COLS.index('CBO')], 0)
        lcom = max(row[CK_COLS.index('LCOM')], 0)
        max_method_loc = max(row[CK_COLS.index('MAX_METHOD_LOC')], 1)
        atfd = max(row[CK_COLS.index('ATFD')], 0)
        tcc = max(row[CK_COLS.index('TCC')], 0.001)
        dit = max(row[CK_COLS.index('DIT')], 0)
        noc = max(row[CK_COLS.index('NOC')], 0)
        private_methods = max(row[CK_COLS.index('PRIVATE_METHODS')], 0)
        
        # Derived features
        features.extend([
            # Complexity ratios
            wmc / methods,                      # Avg complexity per method
            loc / methods,                      # Avg LOC per method
            max_method_loc / loc,               # Longest method ratio
            
            # Data class indicators
            fields / (fields + methods + 1),   # Data-heaviness ratio
            fields / methods if methods > 0 else 0,  # Field to method ratio
            
            # God class indicators
            wmc * loc / 1000,                   # Size-complexity product
            cbo * wmc / 100,                    # Coupling * Complexity
            methods * cbo / 100,                # Methods * Coupling
            
            # Feature envy indicators
            atfd / (methods + 1),               # External access per method
            cbo / (methods + 1),                # Coupling per method
            
            # Long method indicators
            max_method_loc,                     # Absolute longest method
            max_method_loc / methods,           # Longest vs avg
            
            # Cohesion indicators
            lcom / (methods + 1),               # Lack of cohesion per method
            1 / (tcc + 0.001),                  # Inverse TCC
            
            # Encapsulation
            private_methods / (methods + 1),    # Encapsulation ratio
            
            # Hierarchy
            dit + noc,                          # Inheritance depth + children
            
            # Interactions (non-linear features help!)
            np.log1p(loc),                      # Log LOC
            np.log1p(wmc),                      # Log WMC
            np.sqrt(cbo * wmc),                 # Sqrt of coupling-complexity
            loc ** 0.5 * wmc ** 0.5,            # Geometric mean size-complexity
        ])
        X_new.append(features)
    return np.array(X_new)

print()
print("=" * 90)
print("ğŸš€ ULTIMATE PRODUCTION MODEL v2.0")
print("=" * 90)
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOAD DATA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("ğŸ“‚ Loading data...")

with open(os.path.join(BASE, "dataset", "dataset.json")) as f:
    dataset = json.load(f)
with open(os.path.join(BASE, "dataset", "split_info.json")) as f:
    splits = json.load(f)

df = pd.DataFrame(dataset)
train_df = df[df["project"].isin(splits["train"])].copy()
test_df = df[df["project"].isin(splits["test"])].copy()

y_train = np.array([smell_to_idx[s] for s in train_df["true_smell"].values])
y_test = np.array([smell_to_idx[s] for s in test_df["true_smell"].values])

print(f"   Train: {len(train_df):,} | Test: {len(test_df):,}")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PREPARE FEATURES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("âš™ï¸  Preparing features...")

X_train = np.array([extract_ck_features(row) for row in train_df.to_dict('records')])
X_test = np.array([extract_ck_features(row) for row in test_df.to_dict('records')])

X_train = add_derived_features(X_train)
X_test = add_derived_features(X_test)

X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)
X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"   Features: {X_train_scaled.shape[1]}")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLASS WEIGHTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = {i: w for i, w in enumerate(class_weights)}

print("âš–ï¸  Class weights:")
for i, smell in enumerate(SMELLS):
    print(f"      {smell:15s}: {class_weight_dict[i]:.2f}")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BUILD MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("ğŸ”¨ Building models...")

# Model 1: Random Forest
rf = RandomForestClassifier(
    n_estimators=500,
    max_depth=25,
    min_samples_split=3,
    min_samples_leaf=1,
    max_features='sqrt',
    class_weight=class_weight_dict,
    random_state=42,
    n_jobs=-1
)

# Model 2: Gradient Boosting
gb = GradientBoostingClassifier(
    n_estimators=300,
    max_depth=8,
    learning_rate=0.1,
    min_samples_split=5,
    random_state=42
)

# Model 3: XGBoost (if available)
if HAS_XGBOOST:
    # Convert class weights to sample weights format for XGBoost
    sample_weights_train = np.array([class_weight_dict[y] for y in y_train])
    
    xgb = XGBClassifier(
        n_estimators=300,
        max_depth=8,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=42,
        n_jobs=-1,
        eval_metric='mlogloss'
    )
    print("   âœ… XGBoost included")
else:
    xgb = None
    sample_weights_train = None
    print("   âš ï¸  XGBoost not available")

print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAIN INDIVIDUAL MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("ğŸ‹ï¸  Training models...")

# Train RF
print("   Training Random Forest...")
rf.fit(X_train_scaled, y_train)
rf_pred = rf.predict(X_test_scaled)
rf_proba = rf.predict_proba(X_test_scaled)
rf_f1 = f1_score(y_test, rf_pred, average='macro')
print(f"      RF Macro F1: {rf_f1:.3f}")

# Train GB
print("   Training Gradient Boosting...")
gb.fit(X_train_scaled, y_train)
gb_pred = gb.predict(X_test_scaled)
gb_proba = gb.predict_proba(X_test_scaled)
gb_f1 = f1_score(y_test, gb_pred, average='macro')
print(f"      GB Macro F1: {gb_f1:.3f}")

# Train XGBoost
if HAS_XGBOOST and xgb is not None:
    print("   Training XGBoost...")
    xgb.fit(X_train_scaled, y_train, sample_weight=sample_weights_train)
    xgb_pred = xgb.predict(X_test_scaled)
    xgb_proba = xgb.predict_proba(X_test_scaled)
    xgb_f1 = f1_score(y_test, xgb_pred, average='macro')
    print(f"      XGB Macro F1: {xgb_f1:.3f}")
else:
    xgb_proba = None
    xgb_f1 = 0

print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOAD CODEBERT AND CREATE ULTIMATE ENSEMBLE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("ğŸ¤– Loading CodeBERT predictions...")

codebert_path = os.path.join(BASE, "predictions_enhanced.npy")
if not os.path.exists(codebert_path):
    codebert_path = os.path.join(BASE, "predictions.npy")

codebert_logits = np.load(codebert_path)

def softmax_temp(x, temp=1.5):
    exp_x = np.exp((x - np.max(x, axis=1, keepdims=True)) / temp)
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

codebert_proba = softmax_temp(codebert_logits, temp=1.5)
codebert_pred = np.argmax(codebert_proba, axis=1)
codebert_f1 = f1_score(y_test, codebert_pred, average='macro')
print(f"   CodeBERT Macro F1: {codebert_f1:.3f}")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STACKING ENSEMBLE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("ğŸ¯ Creating Ultimate Ensemble...")

# Combine all probabilities
if HAS_XGBOOST and xgb_proba is not None:
    # 4-model ensemble: CodeBERT + RF + GB + XGB
    combined_proba = np.stack([codebert_proba, rf_proba, gb_proba, xgb_proba], axis=0)
    # Learned weights based on individual performance
    weights = np.array([codebert_f1, rf_f1, gb_f1, xgb_f1])
    weights = weights / weights.sum()
    print(f"   Ensemble weights: CB={weights[0]:.2f}, RF={weights[1]:.2f}, GB={weights[2]:.2f}, XGB={weights[3]:.2f}")
else:
    # 3-model ensemble: CodeBERT + RF + GB
    combined_proba = np.stack([codebert_proba, rf_proba, gb_proba], axis=0)
    weights = np.array([codebert_f1, rf_f1, gb_f1])
    weights = weights / weights.sum()
    print(f"   Ensemble weights: CB={weights[0]:.2f}, RF={weights[1]:.2f}, GB={weights[2]:.2f}")

# Weighted average
ensemble_proba = np.average(combined_proba, axis=0, weights=weights)

# Per-class optimization: use different model strengths
CLASS_MODEL_WEIGHTS = {
    0: {'codebert': 0.5, 'ck': 0.5},    # Clean - balanced
    1: {'codebert': 0.2, 'ck': 0.8},    # DataClass - CK is much better (fields ratio)
    2: {'codebert': 0.3, 'ck': 0.7},    # DeadCode - CK better
    3: {'codebert': 0.7, 'ck': 0.3},    # FeatureEnvy - CodeBERT better (semantic)
    4: {'codebert': 0.3, 'ck': 0.7},    # GodClass - CK better (WMC, LOC)
    5: {'codebert': 0.2, 'ck': 0.8},    # LongMethod - CK much better (MAX_METHOD_LOC)
}

# Create class-optimized ensemble
ck_ensemble_proba = (rf_proba + gb_proba) / 2
if HAS_XGBOOST and xgb_proba is not None:
    ck_ensemble_proba = (rf_proba + gb_proba + xgb_proba) / 3

optimal_ensemble_proba = np.zeros_like(codebert_proba)
for i in range(len(y_test)):
    for c in range(6):
        w_cb = CLASS_MODEL_WEIGHTS[c]['codebert']
        w_ck = CLASS_MODEL_WEIGHTS[c]['ck']
        optimal_ensemble_proba[i, c] = w_cb * codebert_proba[i, c] + w_ck * ck_ensemble_proba[i, c]

# Normalize
optimal_ensemble_proba = optimal_ensemble_proba / optimal_ensemble_proba.sum(axis=1, keepdims=True)
optimal_pred = np.argmax(optimal_ensemble_proba, axis=1)

print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RESULTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("=" * 90)
print("ğŸ“Š FINAL RESULTS")
print("=" * 90)
print()

# Basic ensemble
basic_ensemble_pred = np.argmax(ensemble_proba, axis=1)
basic_ensemble_f1 = f1_score(y_test, basic_ensemble_pred, average='macro')
basic_ensemble_acc = (basic_ensemble_pred == y_test).mean()

# Optimal ensemble
optimal_f1 = f1_score(y_test, optimal_pred, average='macro')
optimal_acc = (optimal_pred == y_test).mean()

print(f"{'Model':<30} {'Accuracy':>12} {'Macro F1':>12}")
print("-" * 60)
print(f"{'CodeBERT':<30} {(codebert_pred == y_test).mean()*100:>11.1f}% {codebert_f1:>12.3f}")
print(f"{'Random Forest':<30} {(rf_pred == y_test).mean()*100:>11.1f}% {rf_f1:>12.3f}")
print(f"{'Gradient Boosting':<30} {(gb_pred == y_test).mean()*100:>11.1f}% {gb_f1:>12.3f}")
if HAS_XGBOOST:
    print(f"{'XGBoost':<30} {(xgb_pred == y_test).mean()*100:>11.1f}% {xgb_f1:>12.3f}")
print(f"{'Basic Ensemble':<30} {basic_ensemble_acc*100:>11.1f}% {basic_ensemble_f1:>12.3f}")
print(f"{'ğŸ† OPTIMAL ENSEMBLE':<30} {optimal_acc*100:>11.1f}% {optimal_f1:>12.3f}")
print()

# Per-class breakdown
print("Per-Class F1 Scores:")
print("-" * 80)
print(f"{'Class':<15} {'CodeBERT':>12} {'RF':>12} {'Ensemble':>12} {'Improvement':>15}")
print("-" * 80)

for i, smell in enumerate(SMELLS):
    cb_f1 = f1_score(y_test, codebert_pred, labels=[i], average='macro', zero_division=0)
    rf_cls_f1 = f1_score(y_test, rf_pred, labels=[i], average='macro', zero_division=0)
    ens_f1 = f1_score(y_test, optimal_pred, labels=[i], average='macro', zero_division=0)
    improvement = ((ens_f1 - cb_f1) / (cb_f1 + 0.001)) * 100
    
    imp_str = f"+{improvement:.0f}%" if improvement > 0 else f"{improvement:.0f}%"
    print(f"{smell:<15} {cb_f1:>12.3f} {rf_cls_f1:>12.3f} {ens_f1:>12.3f} {imp_str:>15}")

print()

# Confusion Matrix
print("Optimal Ensemble Confusion Matrix:")
print("-" * 80)
cm = confusion_matrix(y_test, optimal_pred)
print(f"{'':>15}", end="")
for smell in SMELLS:
    print(f"{smell[:10]:>10}", end="")
print()
for i, smell in enumerate(SMELLS):
    print(f"{smell:>15}", end="")
    for j in range(6):
        print(f"{cm[i][j]:>10}", end="")
    print()
print()

# Classification report
print("Classification Report:")
print(classification_report(y_test, optimal_pred, target_names=SMELLS, zero_division=0))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SAVE EVERYTHING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("ğŸ’¾ Saving models...")

# Save predictions
np.save(os.path.join(BASE, "predictions_ultimate.npy"), optimal_ensemble_proba)

# Save models
joblib.dump(rf, os.path.join(BASE, "models", "ultimate_rf.joblib"))
joblib.dump(gb, os.path.join(BASE, "models", "ultimate_gb.joblib"))
if HAS_XGBOOST:
    joblib.dump(xgb, os.path.join(BASE, "models", "ultimate_xgb.joblib"))
joblib.dump(scaler, os.path.join(BASE, "models", "ultimate_scaler.joblib"))

# Save results
results = {
    "optimal_accuracy": float(optimal_acc),
    "optimal_macro_f1": float(optimal_f1),
    "codebert_f1": float(codebert_f1),
    "rf_f1": float(rf_f1),
    "gb_f1": float(gb_f1),
    "xgb_f1": float(xgb_f1) if HAS_XGBOOST else None,
    "per_class_f1": {
        smell: float(f1_score(y_test, optimal_pred, labels=[i], average='macro', zero_division=0))
        for i, smell in enumerate(SMELLS)
    },
    "improvement_over_codebert": float((optimal_f1 - codebert_f1) / codebert_f1 * 100)
}

with open(os.path.join(BASE, "models", "ultimate_results.json"), "w") as f:
    json.dump(results, f, indent=2)

print()
print("=" * 90)
print("âœ… ULTIMATE MODEL COMPLETE!")
print("=" * 90)
print()
print(f"   ğŸ† Final Macro F1: {optimal_f1:.3f}")
print(f"   ğŸ“ˆ Improvement over CodeBERT: +{(optimal_f1 - codebert_f1) / codebert_f1 * 100:.1f}%")
print()
print("   Files saved:")
print("      - predictions_ultimate.npy")
print("      - models/ultimate_rf.joblib")
print("      - models/ultimate_gb.joblib")
if HAS_XGBOOST:
    print("      - models/ultimate_xgb.joblib")
print("      - models/ultimate_scaler.joblib")
print("      - models/ultimate_results.json")
print()
